{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5366a57f",
   "metadata": {},
   "source": [
    "## 1. Predict the net worth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845247c8",
   "metadata": {},
   "source": [
    "### 1.1 Use the automl framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfda710a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6332faf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset\n",
    "tf = pd.read_csv('./clean_data2.csv')\n",
    "train_set, test_set = train_test_split(tf, test_size=0.2)\n",
    "train_set.to_csv(\"./train.csv\")\n",
    "test_set.to_csv(\"./test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f64f821",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/public/zhiyuan/anaconda3/envs/automl/lib/python3.8/site-packages/autogluon/core/dataset.py:54: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  data = load_pd.load(file_path)\n"
     ]
    }
   ],
   "source": [
    "# dataset\n",
    "train_data = TabularDataset('./train.csv')\n",
    "test_data = TabularDataset('./test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77448a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/public/zhiyuan/anaconda3/envs/automl/lib/python3.8/site-packages/pandas/core/arraylike.py:364: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20210727_020832/\"\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20210727_020832/\"\n",
      "AutoGluon Version:  0.2.0\n",
      "Train Data Rows:    973\n",
      "Train Data Columns: 72\n",
      "Preprocessing data ...\n",
      "Warning: Ignoring 153 (out of 973) training examples for which the label value in column 'net worth' is missing\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (11.422731465168827, 0.0, 8.87613, 2.06537)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type argument in fit() (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    127537.06 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.52 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "/public/zhiyuan/anaconda3/envs/automl/lib/python3.8/site-packages/pandas/core/arrays/categorical.py:2630: FutureWarning: The `inplace` parameter in pandas.Categorical.rename_categories is deprecated and will be removed in a future version. Removing unused categories will always return a new Categorical object.\n",
      "  res = method(*args, **kwargs)\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tUnused Original Features (Count: 1): ['psid']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('object', []) : 1 | ['psid']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 69 | ['available_amount', 'next_payment_amount', 'account', 'card', 'credit_card', ...]\n",
      "\t\t('int', [])   :  2 | ['Unnamed: 0', 'Unnamed: 0.1']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 69 | ['available_amount', 'next_payment_amount', 'account', 'card', 'credit_card', ...]\n",
      "\t\t('int', [])   :  2 | ['Unnamed: 0', 'Unnamed: 0.1']\n",
      "\t0.1s = Fit runtime\n",
      "\t71 features in original data used to generate 71 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.47 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.14s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tTo change this, specify the eval_metric argument of fit()\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ...\n",
      "\t-0.7117\t = Validation root_mean_squared_error score\n",
      "\t0.11s\t = Training runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ...\n",
      "\t-0.652\t = Validation root_mean_squared_error score\n",
      "\t0.0s\t = Training runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 0.136\tvalid_set's rmse: 0.616205\n",
      "[2000]\ttrain_set's rmse: 0.0848026\tvalid_set's rmse: 0.608859\n",
      "[3000]\ttrain_set's rmse: 0.0662234\tvalid_set's rmse: 0.605885\n",
      "[4000]\ttrain_set's rmse: 0.0561118\tvalid_set's rmse: 0.605078\n",
      "[5000]\ttrain_set's rmse: 0.049846\tvalid_set's rmse: 0.604932\n",
      "[6000]\ttrain_set's rmse: 0.0452349\tvalid_set's rmse: 0.604907\n",
      "[1000]\ttrain_set's rmse: 0.244317\tvalid_set's rmse: 0.313867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4561\t = Validation root_mean_squared_error score\n",
      "\t8.51s\t = Training runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ...\n",
      "\t-0.4165\t = Validation root_mean_squared_error score\n",
      "\t1.68s\t = Training runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ...\n",
      "\t-0.3364\t = Validation root_mean_squared_error score\n",
      "\t1.07s\t = Training runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ...\n",
      "\t-0.4229\t = Validation root_mean_squared_error score\n",
      "\t19.86s\t = Training runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ...\n",
      "\t-0.5055\t = Validation root_mean_squared_error score\n",
      "\t0.91s\t = Training runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ...\n",
      "\t-0.8351\t = Validation root_mean_squared_error score\n",
      "\t33.63s\t = Training runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ...\n",
      "\t-0.4799\t = Validation root_mean_squared_error score\n",
      "\t17.25s\t = Training runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetMXNet_BAG_L1 ...\n",
      "\tWarning: Exception caused NeuralNetMXNet_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tUnable to import dependency mxnet. A quick tip is to install via `pip install mxnet --upgrade`, or `pip install mxnet_cu101 --upgrade`\n",
      "Fitting model: LightGBMLarge_BAG_L1 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 0.00252806\tvalid_set's rmse: 0.503288\n",
      "[2000]\ttrain_set's rmse: 0.00078366\tvalid_set's rmse: 0.502893\n",
      "[3000]\ttrain_set's rmse: 0.00033338\tvalid_set's rmse: 0.50285\n",
      "[4000]\ttrain_set's rmse: 0.000146542\tvalid_set's rmse: 0.502837\n",
      "[5000]\ttrain_set's rmse: 6.56677e-05\tvalid_set's rmse: 0.502832\n",
      "[6000]\ttrain_set's rmse: 2.93241e-05\tvalid_set's rmse: 0.50283\n",
      "[7000]\ttrain_set's rmse: 1.32354e-05\tvalid_set's rmse: 0.502829\n",
      "[8000]\ttrain_set's rmse: 5.95694e-06\tvalid_set's rmse: 0.502829\n",
      "[9000]\ttrain_set's rmse: 2.6527e-06\tvalid_set's rmse: 0.502829\n",
      "[10000]\ttrain_set's rmse: 1.18613e-06\tvalid_set's rmse: 0.502829\n",
      "[1000]\ttrain_set's rmse: 0.0109388\tvalid_set's rmse: 0.369233\n",
      "[2000]\ttrain_set's rmse: 0.00249983\tvalid_set's rmse: 0.365815\n",
      "[3000]\ttrain_set's rmse: 0.000925871\tvalid_set's rmse: 0.365554\n",
      "[4000]\ttrain_set's rmse: 0.000377912\tvalid_set's rmse: 0.36543\n",
      "[5000]\ttrain_set's rmse: 0.00016367\tvalid_set's rmse: 0.365357\n",
      "[6000]\ttrain_set's rmse: 7.34714e-05\tvalid_set's rmse: 0.365332\n",
      "[7000]\ttrain_set's rmse: 3.22982e-05\tvalid_set's rmse: 0.365325\n",
      "[8000]\ttrain_set's rmse: 1.39759e-05\tvalid_set's rmse: 0.365323\n",
      "[9000]\ttrain_set's rmse: 6.19845e-06\tvalid_set's rmse: 0.365322\n",
      "[10000]\ttrain_set's rmse: 2.79145e-06\tvalid_set's rmse: 0.365322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.3856\t = Validation root_mean_squared_error score\n",
      "\t68.28s\t = Training runtime\n",
      "\t0.84s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-0.3336\t = Validation root_mean_squared_error score\n",
      "\t0.37s\t = Training runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsUnif_BAG_L2 ...\n",
      "\t-0.7117\t = Validation root_mean_squared_error score\n",
      "\t0.01s\t = Training runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L2 ...\n",
      "\t-0.652\t = Validation root_mean_squared_error score\n",
      "\t0.0s\t = Training runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L2 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 0.133451\tvalid_set's rmse: 0.200304\n",
      "[2000]\ttrain_set's rmse: 0.0520361\tvalid_set's rmse: 0.194158\n",
      "[3000]\ttrain_set's rmse: 0.0283701\tvalid_set's rmse: 0.191196\n",
      "[4000]\ttrain_set's rmse: 0.0197759\tvalid_set's rmse: 0.189083\n",
      "[5000]\ttrain_set's rmse: 0.0152394\tvalid_set's rmse: 0.187588\n",
      "[6000]\ttrain_set's rmse: 0.0123516\tvalid_set's rmse: 0.186788\n",
      "[7000]\ttrain_set's rmse: 0.0103716\tvalid_set's rmse: 0.186323\n",
      "[8000]\ttrain_set's rmse: 0.0089128\tvalid_set's rmse: 0.185999\n",
      "[9000]\ttrain_set's rmse: 0.00781254\tvalid_set's rmse: 0.185837\n",
      "[10000]\ttrain_set's rmse: 0.00701304\tvalid_set's rmse: 0.185725\n",
      "[1000]\ttrain_set's rmse: 0.0747824\tvalid_set's rmse: 0.673576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4263\t = Validation root_mean_squared_error score\n",
      "\t11.59s\t = Training runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ...\n",
      "\t-0.3934\t = Validation root_mean_squared_error score\n",
      "\t2.17s\t = Training runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2 ...\n",
      "\t-0.379\t = Validation root_mean_squared_error score\n",
      "\t1.34s\t = Training runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ...\n",
      "\t-0.4076\t = Validation root_mean_squared_error score\n",
      "\t70.79s\t = Training runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ...\n",
      "\t-0.3862\t = Validation root_mean_squared_error score\n",
      "\t0.97s\t = Training runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ...\n",
      "\t-0.6122\t = Validation root_mean_squared_error score\n",
      "\t3.97s\t = Training runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ...\n",
      "\t-0.3734\t = Validation root_mean_squared_error score\n",
      "\t16.93s\t = Training runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetMXNet_BAG_L2 ...\n",
      "\tWarning: Exception caused NeuralNetMXNet_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tUnable to import dependency mxnet. A quick tip is to install via `pip install mxnet --upgrade`, or `pip install mxnet_cu101 --upgrade`\n",
      "Fitting model: LightGBMLarge_BAG_L2 ...\n",
      "\t-0.3725\t = Validation root_mean_squared_error score\n",
      "\t3.56s\t = Training runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ...\n",
      "\t-0.3628\t = Validation root_mean_squared_error score\n",
      "\t0.4s\t = Training runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 275.27s ...\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20210727_020832/\")\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "label='net worth'\n",
    "# do the log transformation\n",
    "train_data[label] = np.log(train_data[label] + 1)\n",
    "predictor = TabularPredictor(label=label, \n",
    "                             eval_metric='root_mean_squared_error')\\\n",
    "            .fit(train_data,num_stack_levels=1,num_bag_folds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "016836e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the prediction\n",
    "# make prediction\n",
    "submission=test_set.copy()\n",
    "predictions = predictor.predict(test_data.drop(columns=[label]))\n",
    "submission['prediction'] = np.exp(predictions) -1\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce348abb",
   "metadata": {},
   "source": [
    "### 1.2 Use the Multilayer Perceptron: written in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "544b1280",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "import torch\n",
    "from sklearn.model_selection import KFold\n",
    "from torch import optim\n",
    "import pickle\n",
    "from torch import nn\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bd306b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    train_data=pd.read_csv(path+'train.csv')\n",
    "    test_data=pd.read_csv(path+'test.csv')\n",
    "    train_data.drop([\"Unnamed: 0.1\",\"Unnamed: 0\",\"psid\"],axis=1,inplace=True)\n",
    "    test_data.drop([\"Unnamed: 0.1\",\"Unnamed: 0\"],axis=1,inplace=True)\n",
    "    return train_data,test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c598ff40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fin_dataset(Dataset):\n",
    "    def __init__(self,X,y):\n",
    "        self.X=X\n",
    "        self.y=y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        X=self.X[index]\n",
    "        y=self.y[index]\n",
    "        #since the load time can be ignore,set t to 0\n",
    "        t=0\n",
    "        return X,y,t  #if self.soft==False:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa70f3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataloaders(train_split, val_split):\n",
    "    train_data = fin_dataset(torch.tensor(train_split.drop([\"net worth\"],axis=1,inplace=False).values,dtype=torch.float32),\n",
    "                             torch.tensor(train_split[\"net worth\"].values,dtype=torch.float32))\n",
    "    train_loader = DataLoader(dataset=train_data, batch_size=16, shuffle=True)\n",
    "#     print(next(iter(train_loader)))\n",
    "\n",
    "    val_data = fin_dataset(torch.tensor(val_split.drop([\"net worth\"],axis=1,inplace=False).values,dtype=torch.float32),\n",
    "                           torch.tensor(val_split[\"net worth\"].values,dtype=torch.float32))\n",
    "    val_loader = DataLoader(dataset=val_data, batch_size=16, shuffle=True)\n",
    "#     print(next(iter(val_loader)))\n",
    "    return train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cb8473d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, model_save,saved_name,patience=7, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            上次验证集损失值改善后等待几个epoch\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "                            如果是True，为每个验证集损失值改善打印一条信息\n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            监测数量的最小变化，以符合改进的要求\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.model_save=model_save\n",
    "        self.saved_name=saved_name\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''\n",
    "        Saves model when validation loss decrease.\n",
    "        验证损失减少时保存模型。\n",
    "        '''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        # torch.save(model.state_dict(), 'checkpoint.pt')     # 这里会存储迄今最优模型的参数\n",
    "        torch.save(model.state_dict(), self.model_save+self.saved_name)                 # 这里会存储迄今最优的模型\n",
    "        self.val_loss_min = val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5773b368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_net():\n",
    "    in_features=69\n",
    "    net = nn.Sequential(nn.Linear(in_features, 30),nn.ReLU(),nn.Linear(30, 1))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb2846ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data,cfg):   \n",
    "    epochs,n_fold,device,loss,model_path,model_save,load_dict=cfg.epochs,cfg.n_fold,cfg.device,cfg.loss,cfg.model_path,cfg.model_save,cfg.load_dict\n",
    "    \n",
    "    folds = KFold(n_splits=n_fold, shuffle=True, random_state=10)\n",
    "    \n",
    "    for fold_i, (train_index, val_index) in enumerate(folds.split(train_data)):\n",
    "        model = get_net()\n",
    "        \n",
    "        if load_dict:\n",
    "            model.load_state_dict(torch.load(model_path+f\"test fold {fold_i}.pt\"))\n",
    "   \n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "        \n",
    "        # scheduler = optim.lr_scheduler.ReduceLROnPlateau(optim, 'min',factor=0.5, verbose = True, min_lr=1e-6, patience = 5)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, 25)\n",
    "\n",
    "        saved_name=f'fold {fold_i}.pt'\n",
    "        early_stopping = EarlyStopping(model_save,saved_name,patience = 10, verbose=True)\n",
    "\n",
    "        model.to(device)\n",
    "        \n",
    "        train_split = train_data.iloc[train_index, :].reset_index(drop=True)\n",
    "        val_split = train_data.iloc[val_index, :].reset_index(drop=True)\n",
    "        train_loader, val_loader = generate_dataloaders(train_split, val_split)\n",
    "        \n",
    "        epoch_trainlosses=[]\n",
    "        epoch_vallosses=[]\n",
    "        for epoch in range(epochs):\n",
    "            epoch_train_start_time=0\n",
    "            epoch_val_start_time=0\n",
    "            epoch_train_end_time = 0\n",
    "            epoch_val_end_time = 0\n",
    "            epoch_train_load_time=0\n",
    "            epoch_val_load_time=0\n",
    "            for (dataset, loader) in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "                \n",
    "                if dataset == \"train\":\n",
    "                        epoch_train_start_time = time()\n",
    "                        torch.set_grad_enabled(True)\n",
    "                        model.train()\n",
    "                else:\n",
    "                        epoch_val_start_time = time()\n",
    "                        torch.set_grad_enabled(False)\n",
    "                        model.eval()\n",
    "                total = 0\n",
    "\n",
    "                for batch_idx, (X,y,data_load_time) in enumerate(loader): \n",
    "                   \n",
    "                    X=X.to(device)\n",
    "                    y=y.to(device)\n",
    "\n",
    "                    y_hat=model(X)\n",
    "                    l=loss(y_hat,y)\n",
    "                    \n",
    "                    total += l.cpu().detach().numpy()\n",
    "                    if(batch_idx%50==0):\n",
    "                        message=\"\"\n",
    "                        message += f\"Epoch {epoch+1}/{epochs} process: {int((batch_idx / len(loader)) * 100)} \"\n",
    "                        message += f'loss: {l.data.item():.4f}'\n",
    "                        print(message)\n",
    "                    \n",
    "                    if dataset == \"train\" :\n",
    "                        epoch_train_load_time += torch.sum(data_load_time)\n",
    "                        optimizer.zero_grad()\n",
    "                        l.backward()\n",
    "                        optimizer.step()\n",
    "                    else:\n",
    "                        epoch_val_load_time += torch.sum(data_load_time)\n",
    "          \n",
    "                epoch_loss = total/ len(loader.dataset)\n",
    "                if dataset == \"train\" :\n",
    "                    epoch_trainlosses.append(epoch_loss)\n",
    "                    epoch_train_end_time=time()\n",
    "                if dataset == 'val':\n",
    "                    epoch_vallosses.append(epoch_loss)\n",
    "                    epoch_val_end_time=time()\n",
    "            \n",
    "            print(f'Epoch: {epoch};')\n",
    "            print(f'Train: data_load_time: {epoch_train_load_time:.4f}; loss: {epoch_trainlosses[-1]}                 batch_total_time: {(epoch_train_end_time - epoch_train_start_time):.4f}')\n",
    "            print(f'Val: data_load_time: {epoch_val_load_time:.4f}; loss: {epoch_vallosses[-1]}                 batch_total_time: {(epoch_val_end_time - epoch_val_start_time):.4f}')\n",
    "            \n",
    "            epoch_valloss=epoch_vallosses[-1]\n",
    "            scheduler.step()\n",
    "            early_stopping(epoch_valloss, model)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "    with open(\"loss.txt\", \"wb\") as f:   #Pickling\n",
    "        pickle.dump(epoch_trainlosses, f)\n",
    "        pickle.dump(epoch_vallosses, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa0f6443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makepred(test_data,cfg):\n",
    "    model_save=cfg.model_save\n",
    "    device=cfg.device\n",
    "\n",
    "    test_set=TensorDataset(torch.tensor(test_data.drop(['net worth','psid'],axis=1,inplace=False).values,dtype=torch.float32),\n",
    "                           torch.tensor(test_data['net worth'].values,dtype=torch.float32))\n",
    "    test_loader=DataLoader(test_set,batch_size=8,shuffle=False)\n",
    "\n",
    "    submission = []\n",
    "    PATH=[\n",
    "        \"fold 0.pt\",\"fold 1.pt\",\"fold 2.pt\",\"fold 3.pt\",\"fold 4.pt\"\n",
    "    ]\n",
    "\n",
    "    model = get_net()\n",
    "    for path in PATH:\n",
    "        print(model_save+path)\n",
    "        model.load_state_dict(torch.load(model_save+path))\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        test_preds = []\n",
    "        with torch.no_grad():\n",
    "            for X in tqdm(test_loader):\n",
    "                test_preds.append(model(X[0]))\n",
    "\n",
    "            test_preds = torch.cat(test_preds)\n",
    "            submission.append(np.argmax(test_preds.cpu().numpy(), axis=1))\n",
    "\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e416058c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "def save_submission(test_data,submission):\n",
    "    submission=np.array(submission)\n",
    "    submission_ensembled = np.mean(np.array(submission),axis=0)\n",
    "    test_data.to_csv(\"submission2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40b86501",
   "metadata": {},
   "outputs": [],
   "source": [
    "class config():\n",
    "    def __init__(self):\n",
    "        self.epochs=1\n",
    "        self.n_fold=5\n",
    "        self.device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.loss=nn.MSELoss()\n",
    "        self.model_path=\"model/\"\n",
    "        self.model_save=\"model_save/\"\n",
    "        self.data_path=\"./\"\n",
    "        self.load_dict=False\n",
    "        \n",
    "cfg=config()\n",
    "\n",
    "train_data,test_data=load_data(cfg.data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76aa23e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>available_amount</th>\n",
       "      <th>next_payment_amount</th>\n",
       "      <th>account</th>\n",
       "      <th>card</th>\n",
       "      <th>credit_card</th>\n",
       "      <th>insurance</th>\n",
       "      <th>investment</th>\n",
       "      <th>savings</th>\n",
       "      <th>current_liability</th>\n",
       "      <th>illiquid_asset</th>\n",
       "      <th>...</th>\n",
       "      <th>Saving</th>\n",
       "      <th>amount</th>\n",
       "      <th>Dining &amp; Beverage</th>\n",
       "      <th>Healthcare</th>\n",
       "      <th>Home</th>\n",
       "      <th>Leisure</th>\n",
       "      <th>Others</th>\n",
       "      <th>Shopping</th>\n",
       "      <th>Transportation</th>\n",
       "      <th>net worth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7629.60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-1324.43</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4186.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7629.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2578.85</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>-578.98</td>\n",
       "      <td>1122.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2578.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1365.82</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1365.82</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-47208.79</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3800.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1365.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12746.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>-62199.98</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-62400.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12746.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1365.82</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1365.82</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-47208.79</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3800.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1365.82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   available_amount  next_payment_amount  account  card  credit_card  \\\n",
       "0              0.00                  0.0      0.0   0.0          0.0   \n",
       "1              0.00                  0.0      0.0   0.0          0.0   \n",
       "2           1365.82                  0.0      0.0   0.0          0.0   \n",
       "3              0.00                  0.0      0.0   0.0          0.0   \n",
       "4           1365.82                  0.0      0.0   0.0          0.0   \n",
       "\n",
       "   insurance  investment   savings  current_liability  illiquid_asset  ...  \\\n",
       "0        0.0         0.0   7629.60                0.0             0.0  ...   \n",
       "1        0.0         0.0   2578.85                0.0             0.0  ...   \n",
       "2        0.0         0.0   1365.82                0.0             0.0  ...   \n",
       "3        0.0         0.0  12746.65                0.0             0.0  ...   \n",
       "4        0.0         0.0   1365.82                0.0             0.0  ...   \n",
       "\n",
       "     Saving    amount  Dining & Beverage  Healthcare  Home  Leisure   Others  \\\n",
       "0  0.500000  -1324.43                0.0         0.0   0.0      0.0  -4186.0   \n",
       "1  0.333333   -578.98             1122.0         0.0   0.0      0.0      0.0   \n",
       "2  1.000000 -47208.79                0.0         0.0   0.0      0.0  -3800.0   \n",
       "3  0.333333 -62199.98                0.0         0.0   0.0      0.0 -62400.0   \n",
       "4  1.000000 -47208.79                0.0         0.0   0.0      0.0  -3800.0   \n",
       "\n",
       "   Shopping  Transportation  net worth  \n",
       "0       0.0             0.0    7629.60  \n",
       "1       0.0             0.0    2578.85  \n",
       "2       0.0             0.0    1365.82  \n",
       "3       0.0             0.0   12746.65  \n",
       "4       0.0             0.0    1365.82  \n",
       "\n",
       "[5 rows x 70 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2d9fda",
   "metadata": {},
   "source": [
    "### Notice\n",
    "1. use the 5 fold cross-validation\n",
    "2. use the early stopping\n",
    "3. use the essemble method\n",
    "4. use CosineAnnealingLR to adapt learning rate\n",
    "5. I just run for an epoch to demostrate the coding skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "887da9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 process: 0 loss: 105127976960.0000\n",
      "Epoch 1/1 process: 0 loss: 1487784320.0000\n",
      "Epoch: 0;\n",
      "Train: data_load_time: 0.0000; loss: 550294474.0359898                 batch_total_time: 0.0659\n",
      "Val: data_load_time: 0.0000; loss: 41372027.323076926                 batch_total_time: 0.0027\n",
      "Validation loss decreased (inf --> 41372027.323077).  Saving model ...\n",
      "Epoch 1/1 process: 0 loss: 841489088.0000\n",
      "Epoch 1/1 process: 0 loss: 227180128.0000\n",
      "Epoch: 0;\n",
      "Train: data_load_time: 0.0000; loss: 50254870.74550129                 batch_total_time: 0.0214\n",
      "Val: data_load_time: 0.0000; loss: 41147150.11282051                 batch_total_time: 0.0024\n",
      "Validation loss decreased (inf --> 41147150.112821).  Saving model ...\n",
      "Epoch 1/1 process: 0 loss: 89586360320.0000\n",
      "Epoch 1/1 process: 0 loss: 760668416.0000\n",
      "Epoch: 0;\n",
      "Train: data_load_time: 0.0000; loss: 363139915.125964                 batch_total_time: 0.0211\n",
      "Val: data_load_time: 0.0000; loss: 43454306.78974359                 batch_total_time: 0.0022\n",
      "Validation loss decreased (inf --> 43454306.789744).  Saving model ...\n",
      "Epoch 1/1 process: 0 loss: 8742069248.0000\n",
      "Epoch 1/1 process: 0 loss: 842678400.0000\n",
      "Epoch: 0;\n",
      "Train: data_load_time: 0.0000; loss: 83037655.78433889                 batch_total_time: 0.0214\n",
      "Val: data_load_time: 0.0000; loss: 34101930.309278354                 batch_total_time: 0.0023\n",
      "Validation loss decreased (inf --> 34101930.309278).  Saving model ...\n",
      "Epoch 1/1 process: 0 loss: 87318544384.0000\n",
      "Epoch 1/1 process: 0 loss: 357430784.0000\n",
      "Epoch: 0;\n",
      "Train: data_load_time: 0.0000; loss: 538286852.4159179                 batch_total_time: 0.0212\n",
      "Val: data_load_time: 0.0000; loss: 52813916.0927835                 batch_total_time: 0.0023\n",
      "Validation loss decreased (inf --> 52813916.092784).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhangzhiyuan/opt/anaconda3/envs/bert/lib/python3.8/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/zhangzhiyuan/opt/anaconda3/envs/bert/lib/python3.8/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([10, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/zhangzhiyuan/opt/anaconda3/envs/bert/lib/python3.8/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([3])) that is different to the input size (torch.Size([3, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/zhangzhiyuan/opt/anaconda3/envs/bert/lib/python3.8/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/zhangzhiyuan/opt/anaconda3/envs/bert/lib/python3.8/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "train(train_data,cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "911a0cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:00<00:00, 2905.88it/s]\n",
      "100%|██████████| 31/31 [00:00<00:00, 4327.19it/s]\n",
      "100%|██████████| 31/31 [00:00<00:00, 4568.16it/s]\n",
      "100%|██████████| 31/31 [00:00<00:00, 6816.07it/s]\n",
      "100%|██████████| 31/31 [00:00<00:00, 6622.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_save/fold 0.pt\n",
      "model_save/fold 1.pt\n",
      "model_save/fold 2.pt\n",
      "model_save/fold 3.pt\n",
      "model_save/fold 4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "submission=makepred(test_data,cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25215105",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_submission(test_data,submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4200ee",
   "metadata": {},
   "source": [
    "## 2. Do the clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54d7cefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from kmodes.kprototypes import KPrototypes\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58a76782",
   "metadata": {},
   "outputs": [],
   "source": [
    "CustomerData = pd.read_csv('./clean_data2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd576bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CustomerData.drop(\"Unnamed: 0\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e449e55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of clusters: 2\n",
      "  cost:  53782.777980201216\n",
      "  average silhouette score:  0.4317722172961733\n",
      "number of clusters: 3\n",
      "  cost:  34469.56768655413\n",
      "  average silhouette score:  0.4867655527326666\n",
      "number of clusters: 4\n",
      "  cost:  29719.964478422\n",
      "  average silhouette score:  0.3955829592609456\n",
      "number of clusters: 5\n",
      "  cost:  25234.151637590432\n",
      "  average silhouette score:  0.5021568217184943\n",
      "number of clusters: 6\n",
      "  cost:  21016.60230669665\n",
      "  average silhouette score:  0.4623483452759594\n",
      "number of clusters: 7\n",
      "  cost:  17580.849753407663\n",
      "  average silhouette score:  0.502506159096952\n"
     ]
    }
   ],
   "source": [
    "Scaler = StandardScaler()\n",
    "X = Scaler.fit_transform(CustomerData.iloc[:,1:])\n",
    "# X = CustomerData.iloc[:,1:].values.astype(str)\n",
    "#finding the optimal cluster_number k\n",
    "for n in range(2,8):\n",
    "    kproto = KPrototypes(n_clusters = n, init = 'Cao')\n",
    "    clusters = kproto.fit_predict(X, categorical = [0])\n",
    "    silhouette = silhouette_score(X[:,1:],clusters)\n",
    "    print('number of clusters:', n)\n",
    "    print('  cost: ',kproto.cost_)\n",
    "    print('  average silhouette score: ',silhouette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "314ac5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers:\n",
      " [[ 3.11725699e-01 -9.57298867e-02 -1.81633973e-01 -2.29854226e+00\n",
      "  -3.25393519e-02 -2.28059015e-02 -4.67152023e-01 -2.29854226e+00\n",
      "  -3.25393519e-02 -4.78766700e-01 -4.73471141e-01 -4.70766904e-01\n",
      "   1.50118008e+00  4.74150002e-01  1.19974879e+00  4.74150002e-01\n",
      "  -4.69500587e-01 -5.43339904e-01  7.31180989e-02  5.30952784e-01\n",
      "  -4.73417388e-01 -1.43537867e+00  1.61102892e-01  1.74504306e+00\n",
      "  -4.79687692e-01 -4.73417388e-01 -4.73556013e-01  4.74150002e-01\n",
      "   1.34828102e+00  1.34828102e+00  1.34828102e+00  1.88203630e+00\n",
      "   1.88203630e+00  1.88203630e+00  1.88203630e+00  1.88203630e+00\n",
      "   1.88203630e+00  1.44473756e+00  1.44473756e+00  1.44473756e+00\n",
      "   1.88203630e+00  1.88203630e+00  1.88203630e+00  1.88203688e+00\n",
      "   1.88203688e+00  1.88203688e+00  1.88203454e+00  1.88203630e+00\n",
      "   1.88203630e+00  1.88203630e+00  1.46469819e-01 -4.71748903e-01\n",
      "   1.83193363e-01  1.28314802e+00 -5.04528062e-01 -4.70356038e-01\n",
      "   1.88203630e+00  2.50830766e-01 -4.70356038e-01 -6.34986193e-01\n",
      "   8.77222604e-01 -3.16376127e+00 -2.84290161e+00 -3.59846644e+00\n",
      "  -3.01546346e+00 -1.13746007e+00 -3.56544172e+00 -3.15764384e+00\n",
      "  -1.13826802e+00 -3.34822496e-01]\n",
      " [-1.74734294e-01 -8.60605775e-02 -2.40884986e-01  3.84360019e-01\n",
      "   1.48744932e-03  1.04251071e-03  2.12877960e-01  3.84360022e-01\n",
      "   1.48744932e-03  5.94163904e-02  2.02827835e+00  2.02841546e+00\n",
      "   1.19719974e-01 -2.02828446e+00  6.59642991e-01 -2.02828446e+00\n",
      "   2.02840321e+00  1.97598016e+00 -4.41469753e-01  1.30844025e+00\n",
      "   2.02827764e+00 -5.58522203e-01 -1.55893611e+00 -1.44045752e-01\n",
      "   2.02730280e+00  2.02827764e+00  2.02827947e+00 -2.02828446e+00\n",
      "  -6.57666234e-01 -6.57666234e-01 -6.57666234e-01 -5.24593804e-01\n",
      "  -5.24593804e-01 -5.24593804e-01 -5.24593804e-01 -5.24593804e-01\n",
      "  -5.24593804e-01  4.13268543e-01  4.13268543e-01  4.13268543e-01\n",
      "  -5.24593805e-01 -5.24593805e-01 -5.24593805e-01 -5.24617042e-01\n",
      "  -5.24617042e-01 -5.24617042e-01 -5.24735725e-01 -5.24593804e-01\n",
      "  -5.24593804e-01 -5.24593804e-01  1.40051577e+00  2.02843004e+00\n",
      "  -1.86155673e+00 -6.50576909e-01  2.00661079e+00 -5.11256564e-01\n",
      "  -5.24593804e-01 -3.25245894e-01 -5.11256564e-01 -8.92933441e-01\n",
      "   2.20654261e-01  2.86433261e-01  2.96194077e-01  2.96733635e-01\n",
      "   2.98967694e-01  3.29040106e-01  2.97677693e-01  3.00374567e-01\n",
      "   1.72218101e-01 -3.34822496e-01]\n",
      " [ 7.02232109e-02  5.90260691e-02  1.42102913e-01 -8.47361372e-03\n",
      "   8.21740391e-03  5.75934348e-03 -6.90120707e-03 -8.47362128e-03\n",
      "   8.21740391e-03  6.60542924e-02 -4.72982424e-01 -4.74015451e-01\n",
      "  -5.92395929e-01  4.72734663e-01 -6.52553799e-01  4.72734663e-01\n",
      "  -4.74477032e-01 -4.30692202e-01  1.17563922e-01 -6.13695403e-01\n",
      "  -4.73001964e-01  7.06697576e-01  4.38551502e-01 -5.95923003e-01\n",
      "  -4.70404270e-01 -4.73001964e-01 -4.72951571e-01  4.72734663e-01\n",
      "  -2.86332690e-01 -2.86332690e-01 -2.86332690e-01 -5.24593804e-01\n",
      "  -5.24593804e-01 -5.24593804e-01 -5.24593804e-01 -5.24593804e-01\n",
      "  -5.24593804e-01 -6.63400763e-01 -6.63400763e-01 -6.63400763e-01\n",
      "  -5.24593804e-01 -5.24593804e-01 -5.24593804e-01 -5.24586334e-01\n",
      "  -5.24586334e-01 -5.24586334e-01 -5.24548100e-01 -5.24593804e-01\n",
      "  -5.24593804e-01 -5.24593804e-01 -5.01818059e-01 -4.73658936e-01\n",
      "   5.27137919e-01 -2.64884565e-01 -4.54746855e-01  3.34653535e-01\n",
      "  -5.24593804e-01  1.11490513e-02  3.34653535e-01  5.19956142e-01\n",
      "  -2.02983353e-01  1.24607084e-01  8.62695055e-02  1.64779506e-01\n",
      "   1.02315964e-01 -7.06235250e-02  1.60663775e-01  1.16161767e-01\n",
      "   5.32442262e-02 -3.34822496e-01]\n",
      " [-1.65810614e-01 -8.26758822e-02 -1.76822452e-01  4.03144117e-01\n",
      "  -2.04990991e-02 -1.43672326e-02 -5.78390652e-02  4.03144141e-01\n",
      "  -2.04990991e-02 -1.42786153e-01 -4.71116564e-01 -4.68424187e-01\n",
      "   1.51329057e+00  4.71797431e-01  1.20740277e+00  4.71797431e-01\n",
      "  -4.67155282e-01 -5.40932686e-01  5.91724327e-02  5.38359632e-01\n",
      "  -4.71062671e-01 -1.44255916e+00  1.57148301e-01  1.74991752e+00\n",
      "  -4.77280111e-01 -4.71062671e-01 -4.71201656e-01  4.71797431e-01\n",
      "   1.35187571e+00  1.35187571e+00  1.35187571e+00  1.88549108e+00\n",
      "   1.88549108e+00  1.88549108e+00  1.88549108e+00  1.88549108e+00\n",
      "   1.88549108e+00  1.45021516e+00  1.45021516e+00  1.45021516e+00\n",
      "   1.88549108e+00  1.88549108e+00  1.88549108e+00  1.88549062e+00\n",
      "   1.88549062e+00  1.88549062e+00  1.88548988e+00  1.88549108e+00\n",
      "   1.88549108e+00  1.88549108e+00  1.53544225e-01 -4.69408896e-01\n",
      "   1.78158640e-01  1.28748197e+00 -5.01929684e-01 -4.67209844e-01\n",
      "   1.88549108e+00  2.52301471e-01 -4.67209844e-01 -6.44023638e-01\n",
      "   1.76399582e-01  3.36499099e-01  3.51753292e-01  3.31656979e-01\n",
      "   3.51014082e-01  2.81708541e-01  3.33927207e-01  3.49128834e-01\n",
      "   8.33982898e-03 -3.34822496e-01]]\n"
     ]
    }
   ],
   "source": [
    "kproto = KPrototypes(n_clusters = 4, init = 'Cao')\n",
    "clusters = kproto.fit_predict(X, categorical = [0])\n",
    "print('Cluster Centers:\\n', kproto.cluster_centroids_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ddf27e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e004df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         available_amount  next_payment_amount       account          card  \\\n",
      "Cluster                                                                      \n",
      "0            1.161367e+09         33896.618495  1.893023e+07  4.974229e+03   \n",
      "1            2.117244e+08        135232.166494  1.873714e+07  1.233017e+06   \n",
      "2            2.234869e+08         35943.757404  1.844892e+07  1.214064e+06   \n",
      "3            2.152985e+08         36782.486548  2.054903e+07  5.311034e+03   \n",
      "4            2.040754e+08         33000.801046  1.842773e+07  6.053126e+04   \n",
      "5            6.070744e+08        128602.866304  5.003068e+07  1.631195e+07   \n",
      "6            1.948454e+08        132116.554329  1.713133e+07  1.127419e+06   \n",
      "\n",
      "          credit_card     insurance    investment       savings  \\\n",
      "Cluster                                                           \n",
      "0       -7.186996e+05  1.918524e+07  5.442865e+06  4.514125e+08   \n",
      "1       -1.495762e+08  1.648758e+07  4.677578e+06  5.333631e+07   \n",
      "2       -2.314339e+06  1.623395e+07  4.605628e+06  1.316029e+08   \n",
      "3       -7.798962e+05  2.082593e+07  5.908302e+06  1.526208e+08   \n",
      "4       -6.997033e+05  1.867595e+07  5.298386e+06  1.828640e+08   \n",
      "5       -2.370514e+07  1.741252e+07  4.939970e+06  9.158294e+07   \n",
      "6       -1.366067e+08  1.507452e+07  4.276714e+06  5.544170e+07   \n",
      "\n",
      "         current_liability  illiquid_asset  liquid_asset  budget_cut_weekly  \\\n",
      "Cluster                                                                       \n",
      "0            -7.187086e+05    1.918524e+07  6.175442e+08       2.033208e+09   \n",
      "1            -1.495762e+08    1.648758e+07  1.173839e+08       3.195810e+11   \n",
      "2            -2.314347e+06    1.623395e+07  2.154517e+08       9.521399e+10   \n",
      "3            -7.799060e+05    2.082593e+07  2.442821e+08       3.692083e+08   \n",
      "4            -6.997120e+05    1.867595e+07  2.773783e+08       7.302611e+12   \n",
      "5            -2.370515e+07    1.741252e+07  2.812735e+08       2.131300e+11   \n",
      "6            -1.366067e+08    1.507452e+07  1.157546e+08       8.846038e+10   \n",
      "\n",
      "         wealth_growth  monthly_max_saving  budget_daily  feasible  \\\n",
      "Cluster                                                              \n",
      "0             9.400524       -1.822043e+06 -3.092746e+06  0.315498   \n",
      "1            48.203704       -1.914919e+07 -6.520872e+09  0.324181   \n",
      "2            21.356648        2.338730e+07 -1.883138e+09  0.463097   \n",
      "3             9.400101       -3.232702e+06 -5.192303e+06  0.314038   \n",
      "4           899.391067        6.399228e+06 -1.490184e+11  0.430820   \n",
      "5            33.229541        1.545096e+06 -4.345577e+09  0.401746   \n",
      "6            20.537205        2.323823e+07 -1.745422e+09  0.462638   \n",
      "\n",
      "         budget_monthly  saved_amount   diff2target  monthly_min_expense  \\\n",
      "Cluster                                                                    \n",
      "0         -2.677390e+09  2.744297e+05  1.892818e+10         1.040008e+07   \n",
      "1         -5.868679e+12  6.501972e+09  1.144275e+10         1.106560e+07   \n",
      "2         -1.694718e+12  2.101957e+09  2.436633e+09         2.636318e+06   \n",
      "3         -4.566991e+09  1.493911e+05  1.040668e+09         2.407490e+06   \n",
      "4         -1.341165e+14  1.491350e+11  1.477798e+11         6.376467e+05   \n",
      "5         -3.910913e+12  3.985927e+09  1.538068e+10         2.386767e+05   \n",
      "6         -1.570774e+12  1.964514e+09  2.298342e+09         2.690058e+06   \n",
      "\n",
      "         completion_rate  budget_cut_monthly  difficulty_idx        delta  \\\n",
      "Cluster                                                                     \n",
      "0               0.267052        3.735411e+10       33.620150  6595.675907   \n",
      "1               0.275020        5.869812e+12       32.745942  3405.158135   \n",
      "2               0.344150        1.752797e+12       14.449982  2829.631385   \n",
      "3               0.266286        6.773240e+09       33.332918  3403.379720   \n",
      "4               0.377655        1.341284e+14       20.859926   268.184490   \n",
      "5               0.333517        3.914879e+12       25.827293  2422.290453   \n",
      "6               0.343833        1.628746e+12       14.502602  2835.493817   \n",
      "\n",
      "         monthly_avg_income    amount_bgt  budget_cut_daily  \\\n",
      "Cluster                                                       \n",
      "0              1.481494e+07  1.420014e+11      4.162251e+07   \n",
      "1              5.540685e+05  3.969803e+11      6.522131e+09   \n",
      "2              2.483050e+07  1.263781e+11      1.947670e+09   \n",
      "3              9.450440e+05  2.997283e+09      7.643774e+06   \n",
      "4              6.661517e+06  8.942937e+12      1.490317e+11   \n",
      "5              1.741806e+06  3.282125e+11      4.349984e+09   \n",
      "6              2.478364e+07  1.179482e+11      1.809835e+09   \n",
      "\n",
      "         current_goal_budget_cut_by_day  budget_weekly  Others_weekly  \\\n",
      "Cluster                                                                 \n",
      "0                          4.156088e+07  -1.464234e+08     204.698601   \n",
      "1                          6.522219e+09  -3.195176e+11     181.469729   \n",
      "2                          1.936264e+09  -9.226865e+10    6714.373249   \n",
      "3                          7.632615e+06  -2.493016e+08    3046.986379   \n",
      "4                          1.490350e+11  -7.301898e+12     146.835866   \n",
      "5                          4.349244e+09  -2.129282e+11     106.023203   \n",
      "6                          1.798445e+09  -8.552055e+10    6702.621825   \n",
      "\n",
      "         Others_monthly  Others_daily  Leisure_weekly  Leisure_monthly  \\\n",
      "Cluster                                                                  \n",
      "0           3204.273571      9.007928       21.027146        90.116339   \n",
      "1           2777.620821      8.533869       21.027146        90.116339   \n",
      "2         122769.726297    141.858431     3893.128109     71210.338106   \n",
      "3          55409.559290     67.013801       21.027146        90.116339   \n",
      "4           2141.488653      7.827056       21.027146        90.116339   \n",
      "5           1391.868309      6.994144       21.027146        90.116339   \n",
      "6         122553.883814    141.618606     3887.577583     71108.389672   \n",
      "\n",
      "         Leisure_daily  Transportation_weekly  Transportation_monthly  \\\n",
      "Cluster                                                                 \n",
      "0             3.003878              15.535104               66.579016   \n",
      "1             3.003878              15.535104               66.579016   \n",
      "2            82.026347            2129.093944            38887.047509   \n",
      "3             3.003878              15.535104               66.579016   \n",
      "4             3.003878              15.535104               66.579016   \n",
      "5             3.003878              15.535104               66.579016   \n",
      "6            81.913071            2126.064229            38831.399679   \n",
      "\n",
      "         Transportation_daily  Financials_weekly  Financials_monthly  \\\n",
      "Cluster                                                                \n",
      "0                    2.219301      266643.318571        4.892666e+06   \n",
      "1                    2.219301       12616.648222        2.268703e+05   \n",
      "2                   45.353154      310200.862313        5.692703e+06   \n",
      "3                    2.219301        3734.534622        6.372939e+04   \n",
      "4                    2.219301      178081.761017        3.266025e+06   \n",
      "5                    2.219301       39627.082783        7.229803e+05   \n",
      "6                   45.291324      309497.897258        5.679791e+06   \n",
      "\n",
      "         Financials_daily  Dining & Beverage_weekly  \\\n",
      "Cluster                                               \n",
      "0             5483.996885                 24.868895   \n",
      "1              299.779123                 24.868920   \n",
      "2             6372.926349               5441.122368   \n",
      "3              118.511498                 24.868900   \n",
      "4             3676.618159                 24.868895   \n",
      "5              851.012481                 24.868895   \n",
      "6             6358.580124               5433.358352   \n",
      "\n",
      "         Dining & Beverage_monthly  Dining & Beverage_daily  \\\n",
      "Cluster                                                       \n",
      "0                       106.580981                 3.552699   \n",
      "1                       106.581453                 3.552700   \n",
      "2                     99588.787637               114.088484   \n",
      "3                       106.581069                 3.552699   \n",
      "4                       106.580980                 3.552699   \n",
      "5                       106.580978                 3.552699   \n",
      "6                     99446.183260               113.930035   \n",
      "\n",
      "         Healthcare_weekly  Healthcare_monthly  Healthcare_daily  \\\n",
      "Cluster                                                            \n",
      "0                 0.144121            0.617664          0.020589   \n",
      "1                 0.144136            0.617931          0.020589   \n",
      "2                 0.326008            3.958443          0.024301   \n",
      "3                 0.144124            0.617714          0.020589   \n",
      "4                 0.144121            0.617663          0.020589   \n",
      "5                 0.144121            0.617662          0.020589   \n",
      "6                 0.325747            3.953655          0.024295   \n",
      "\n",
      "         Shopping_weekly  Home_weekly  Home_monthly  Home_daily  Completed  \\\n",
      "Cluster                                                                      \n",
      "0              35.406857     1.169018      5.010077    0.167003   0.224187   \n",
      "1              35.390655     1.169018      5.010077    0.167003   0.230651   \n",
      "2           10937.043324    13.137208    224.833968    0.411251   0.268691   \n",
      "3              37.389957     1.169018      5.010077    0.167003   0.223651   \n",
      "4              35.366414     1.169018      5.010077    0.167003   0.309461   \n",
      "5              35.341837     1.169018      5.010077    0.167003   0.279698   \n",
      "6           10921.414515    13.120052    224.518858    0.410901   0.268463   \n",
      "\n",
      "           Failed  In progress  Dining_bgt  Education_bgt  Healthcare_bgt  \\\n",
      "Cluster                                                                     \n",
      "0        0.094330     1.020864    0.168736       0.202458        0.064935   \n",
      "1        0.098244     1.001561    0.168148       0.217833        0.066920   \n",
      "2        0.095486     0.934629    0.283156       0.206176        0.065646   \n",
      "3        0.094330     1.021876    0.185834       0.201345        0.064935   \n",
      "4        0.184097     0.697179    0.165647       0.558147        0.064935   \n",
      "5        0.096727     0.911573    0.199976       0.228148        0.097032   \n",
      "6        0.095404     0.935211    0.282894       0.205814        0.065595   \n",
      "\n",
      "         Home_bgt  Leisure_bgt  Property_bgt    Saving        amount  \\\n",
      "Cluster                                                                \n",
      "0        0.051829     0.255248      0.064935  0.401890 -4.901543e+09   \n",
      "1        0.051829     0.072097      0.066920  0.728390  4.933775e+08   \n",
      "2        0.075354     0.094581      0.065646  0.433855 -1.937476e+08   \n",
      "3        0.051829     0.070541      0.064935  0.732760 -6.975347e+08   \n",
      "4        0.051829     0.071716      0.064935  0.395133 -1.506189e+08   \n",
      "5        0.051829     0.071198      0.097032  0.487746 -1.284361e+08   \n",
      "6        0.075320     0.094523      0.065595  0.435188  7.059249e+08   \n",
      "\n",
      "         Dining & Beverage   Healthcare           Home       Leisure  \\\n",
      "Cluster                                                                \n",
      "0             1.687323e+05   -11.429088    -113.960559 -3.321749e+03   \n",
      "1            -1.959745e+08 -3384.075066 -237204.661269 -2.680084e+08   \n",
      "2             2.874818e+05   -11.429088    -113.960559 -3.321749e+03   \n",
      "3             1.641947e+06   -11.429088    -113.960559 -3.321749e+03   \n",
      "4             1.641784e+05   -11.429088    -113.960559 -3.321749e+03   \n",
      "5             2.114404e+06   -11.429088    -113.960559 -3.321749e+03   \n",
      "6            -2.471727e+08 -3384.075066 -464134.331948 -3.014846e+08   \n",
      "\n",
      "               Others      Shopping  Transportation     net worth  \n",
      "Cluster                                                            \n",
      "0       -2.012767e+07 -5.313270e+03   -1.602871e+03  8.205190e+08  \n",
      "1       -3.814988e+08 -2.376638e+08   -5.909056e+07 -2.768460e+08  \n",
      "2       -3.703305e+07 -5.044446e+03   -1.602871e+03  3.010552e+08  \n",
      "3       -1.961345e+08 -2.032100e+04   -1.602871e+03  3.523992e+08  \n",
      "4       -1.959349e+07 -5.266880e+03   -1.602871e+03  3.896915e+08  \n",
      "5       -1.506574e+07 -5.151798e+03   -1.602871e+03  3.207975e+08  \n",
      "6       -3.802330e+08 -4.446346e+08   -7.391675e+07 -2.420040e+08  \n"
     ]
    }
   ],
   "source": [
    "#scaling back to original values and retrieving all attributes\n",
    "Clustered = pd.DataFrame(data=Scaler.inverse_transform(CustomerData.iloc[:,1:]),columns=CustomerData.columns[1:])\n",
    "Clustered['Cluster'] = clusters\n",
    "print(Clustered.groupby(['Cluster']).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25059f91",
   "metadata": {},
   "source": [
    "### Analyse the clustering result\n",
    "1. cluster 0 spend lots of money, they have the least available_amount, their net worth is negetive, probability end up in debt\n",
    "2. cluster 1 has the most fast speed of money growth, but they do not use money much so the budget is not high\n",
    "3. cluster 2 has the most available_amount of money, but they do not use money much so the budget is not high\n",
    "4. cluster 3 seem to be the most rich group, they have the second most available_amount, and they budget is very high, which indicate they are super rich"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
